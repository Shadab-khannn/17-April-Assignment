{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b12ab5-0058-49a2-ba99-d14e31e17d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9a897-e30e-4e51-b1da-b939f7189582",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3010add2-a977-46fd-91b1-cbb05ad4aa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression is a machine learning technique used in regression tasks. \n",
    "It is an analytical technique that is designed to explore the relationship between two or more variables (X, and Y).\n",
    "Its analytical output identifies important factors (X i) impacting the dependent variable (y) and the nature of the relationship\n",
    "between each of these factors and the dependent variable.\n",
    "It gives a prediction model in the form of an ensemble of weak prediction models, which are typically decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77051853-aa15-459d-be4a-61da31d36c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2040d-0cb9-4508-96b6-3d24d1a055c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc9a6b-1267-44fd-a01e-00f465c98c6a",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337b5866-67a7-4fa5-b637-4bfef38fa818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 2.193280052778249e-16\n",
      "Test MSE: 21.716666863767966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        for i in range(n_estimators):\n",
    "            self.trees.append(DecisionTreeRegressor(max_depth=max_depth))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_pred = np.full(np.shape(y), np.mean(y, axis=0))\n",
    "        for i in range(self.n_estimators):\n",
    "            gradient = y - y_pred\n",
    "            self.trees[i].fit(X, gradient)\n",
    "            update = self.trees[i].predict(X)\n",
    "            y_pred += self.learning_rate * update\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(np.shape(X)[0], np.mean(y_train, axis=0))\n",
    "        for i in range(self.n_estimators):\n",
    "            update = self.trees[i].predict(X)\n",
    "            y_pred += self.learning_rate * update\n",
    "        return y_pred\n",
    "\n",
    "# Example usage:\n",
    "X_train = np.array([[1], [2], [3], [4], [5]])\n",
    "y_train = np.array([2.1, 3.9, 5.9, 8.1, 10.2])\n",
    "X_test = np.array([[6], [7], [8]])\n",
    "y_test = np.array([12.3, 14.5, 16.7])\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "print(\"Train MSE:\", np.mean((y_train - y_pred_train) ** 2))\n",
    "print(\"Test MSE:\", np.mean((y_test - y_pred_test) ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f064fc08-2809-484a-b388-c1cf98843470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e9e40-aa0d-4f52-a4c5-7c753d2317bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca6a7e-de77-43c4-9a73-603c1cc5bd3d",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "503a5179-d4ba-469b-9939-099fd5bf3e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.786 (0.069)\n"
     ]
    }
   ],
   "source": [
    "# perceptron default hyperparameters for binary classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import Perceptron\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=5, n_informative=2, n_redundant=1, random_state=1)\n",
    "# define model\n",
    "model = Perceptron()\n",
    "# define evaluation procedure\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate model\n",
    "scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report result\n",
    "print('Mean Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515c9c7-43dc-4d43-bab7-ea7f777027f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d319b4-20a5-4739-a310-734999dcf6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c457847-eeeb-44f6-9c51-1a3d03be81e2",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80192a66-7922-402b-8bf4-d768fa94e32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Gradient Boosting, a weak learner is a model that is only slightly better than random guessing.\n",
    "It is combined with many other weak learners to form a robust ensemble model to make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f58cbe-e48d-4a34-96d9-7ca7b8b09efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c1bf6-81d8-49ff-af45-685eef5a2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e7088-fc56-4405-bb5b-6336cdab9332",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4860a-7ca8-4073-879d-4841084401bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind Gradient Boosting is to iteratively add weak learners to the ensemble model while focusing on the\n",
    "mistakes made by the previous models. \n",
    "The algorithm tries to minimize the loss function by adding weak learners that can correct the errors made by the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e7ed21-7f5e-4b85-b855-4cf81615e73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76927f9f-e0a0-4fa3-9133-0511ad2cf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60867f9-d903-44e4-83aa-5e8a254da373",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64e212-05de-4c78-b9a4-f7fe8499231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding weak learners to the ensemble model\n",
    "while focusing on the mistakes made by the previous models. \n",
    "The algorithm tries to minimize the loss function by adding weak learners that can correct the errors made by the previous models.\n",
    "  \n",
    "    Gradient Boosting algorithm builds an ensemble of weak learners by using additive (sequential) expansions and gradient descent\n",
    "minimization. Starting with one weak learner, the algorithm adds new weak learners one by one, trained based on the previous learners\n",
    "and according to the gradient descent direction of the loss function. The algorithm attempts to minimize a loss function by training a \n",
    "sequence of models iteratively ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ddd7d3-d41e-49ea-bd3d-d1b1a011ae3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdda636-10d6-4dbb-8f75-644c7208c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63023d81-a73c-43b6-b775-b7f51c458365",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ee295-aa88-4610-884e-f46a6dbb1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting is an ensemble technique that combines multiple weak models to create a strong model. \n",
    "The Gradient Boosting ensemble technique consists of three simple steps:\n",
    "\n",
    "1. An initial model F0 is defined to predict the target variable y. This model will be associated with a residual (y – F0)\n",
    "\n",
    "2. A new model h1 is fit to the residuals from the previous step.\n",
    "\n",
    "3. Now, F0 and h1 are combined to give F1, the boosted version of F0. The mean squared error from F1 will be lower than that from F0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
